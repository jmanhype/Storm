#!/usr/bin/env python3
"""
STORM CLI - Synthesis of Topic Outline through Retrieval and Multi-perspective generation

A comprehensive research and article generation tool powered by OpenRouter.
"""

import logging
import json
import os
import sys
import argparse
from datetime import datetime
from pathlib import Path
from pydantic import BaseModel
import dspy
from utils import fetch_wikipedia_links, fetch_table_of_contents

# Configure logging
def setup_logging(verbose=False):
    level = logging.DEBUG if verbose else logging.INFO
    logging.basicConfig(
        level=level,
        format='%(asctime)s - %(levelname)s - %(message)s',
        datefmt='%H:%M:%S'
    )

# Get OpenRouter API key from environment
openrouter_api_key = os.environ.get("OPENROUTER_API_KEY")
if not openrouter_api_key:
    print("‚ùå ERROR: OPENROUTER_API_KEY environment variable not found")
    print("\nPlease set it with:")
    print("  export OPENROUTER_API_KEY='your-key-here'")
    sys.exit(1)

# Initialize DSPy settings with OpenRouter
lm = dspy.LM(
    model="openrouter/anthropic/claude-3-haiku",
    api_key=openrouter_api_key,
    api_base="https://openrouter.ai/api/v1"
)
dspy.settings.configure(lm=lm)

class LinkData(BaseModel):
    links: list[str]
    def to_json(self):
        return json.dumps(self.links)

class TableOfContents(BaseModel):
    sections: list[str]
    def to_json(self):
        return json.dumps(self.sections)

class ConversationSignature(dspy.Signature):
    topic = dspy.InputField(desc="Main topic")
    perspective = dspy.InputField(desc="Perspective for the conversation")
    conversation_history = dspy.InputField(desc="Previous conversation history", optional=True)
    question = dspy.OutputField(desc="Generated question")
    answer = dspy.OutputField(desc="Synthesized answer")

class ResearchSignature(dspy.Signature):
    topic: str = dspy.InputField(desc="The topic to research")
    related_topics: str = dspy.OutputField(desc="Wikipedia links related to the topic")
    table_of_contents: str = dspy.OutputField(desc="Table of contents for each related topic")

class GenerateTableOfContentsSignature(dspy.Signature):
    topic: str = dspy.InputField(desc="The main topic")
    related_topics: str = dspy.InputField(desc="Related topics and subtopics")
    rationale: str = dspy.InputField(desc="Rationale for generating the table of contents")
    table_of_contents: str = dspy.OutputField(desc="Generated table of contents")

class PerspectiveSignature(dspy.Signature):
    topic = dspy.InputField(desc="The main topic for which perspectives are needed")
    perspectives = dspy.OutputField(desc="Generated list of perspectives")

class CombinedSignature(dspy.Signature):
    topic = dspy.InputField(desc="Main topic for outline creation")
    content = dspy.InputField(desc="Content gathered from conversations")
    prompt = dspy.InputField(desc="Prompt for generating the article")
    full_article = dspy.OutputField(desc="Completed article text")

class FullArticleCreationModule(dspy.Module):
    def __init__(self, target_words=800, max_iterations=10):
        super().__init__()
        self.process_article = dspy.ChainOfThought(CombinedSignature)
        self.target_words = target_words
        self.max_iterations = max_iterations

    def generate_full_article(self, topic, conversation_history, prompt):
        """
        Generate article iteratively, section by section to avoid repetition.
        Each iteration writes a NEW section, not repeating previous content.
        """
        content = " ".join([answer for _, answer in conversation_history])
        full_article = ""
        iterations = 0
        sections_written = []

        logging.info(f"üéØ Target: {self.target_words} words")

        while len(full_article.split()) < self.target_words and iterations < self.max_iterations:
            iterations += 1
            current_length = len(full_article.split())
            remaining_words = self.target_words - current_length

            # Build directive for this iteration to write NEW content
            if iterations == 1:
                section_prompt = f"""Write the INTRODUCTION section for '{topic}'.
- Provide background and context
- Define key terms
- Preview what the article will cover
Target: {min(remaining_words, 200)} words."""

            elif iterations == 2:
                section_prompt = f"""Write the MAIN TECHNOLOGIES/METHODS section for '{topic}'.
- Focus on HOW it works, technical details, mechanisms
- Include specific examples and technologies
- DO NOT repeat benefits/challenges from intro
- DO NOT redefine terms already covered
Target: {min(remaining_words, 250)} words."""

            elif iterations == 3:
                section_prompt = f"""Write the REAL-WORLD APPLICATIONS section for '{topic}'.
- Focus on concrete examples, case studies, current uses
- Industries, companies, projects using this
- Specific numerical data if available
- DO NOT repeat general concepts already covered
Target: {min(remaining_words, 250)} words."""

            elif iterations == 4:
                section_prompt = f"""Write the CHALLENGES AND FUTURE section for '{topic}'.
- Technical/practical obstacles
- Current research directions
- What's on the horizon
- DO NOT re-list benefits or basic applications already mentioned
Target: {min(remaining_words, 200)} words."""

            else:
                section_prompt = f"""Write a brief CONCLUSION for '{topic}'.
- Synthesize the big picture
- Future outlook
- DO NOT repeat specific details, examples, or lists from earlier sections
- Keep it high-level and forward-looking
Target: {min(remaining_words, 150)} words."""

            # Include what's already written so LLM knows what NOT to repeat
            if full_article:
                section_prompt += f"\n\n=== CRITICAL: DO NOT REPEAT ANY OF THIS ===\n{full_article[-600:]}"

            # Generate new section
            prediction = self.process_article(
                topic=topic,
                content=content,
                prompt=section_prompt
            )

            if hasattr(prediction, 'full_article'):
                generated_text = prediction.full_article.strip()
                word_count = len(generated_text.split())

                if word_count < 50:
                    logging.warning(f"‚ö†Ô∏è  Section too short ({word_count} words), skipping")
                    continue

                # Add the new section
                full_article += "\n\n" + generated_text
                sections_written.append(f"Section {iterations}: {word_count} words")
                logging.info(f"‚úì Section {iterations}: +{word_count} words (total: {len(full_article.split())})")

                if len(full_article.split()) >= self.target_words:
                    logging.info(f"üéâ Target reached!")
                    break
            else:
                logging.error("‚ùå Failed to generate section")
                break

        final_word_count = len(full_article.split())
        logging.info(f"üìù Complete: {final_word_count} words in {iterations} iterations")

        return full_article.strip(), final_word_count, iterations

class StormResearchModule(dspy.Module):
    def __init__(self, target_words=800, max_iterations=10):
        super().__init__()
        self.research_module = dspy.ChainOfThought(ResearchSignature)
        self.generate_toc_module = dspy.ChainOfThought(GenerateTableOfContentsSignature)
        self.conversation_module = dspy.ChainOfThought(ConversationSignature)
        self.perspective_predict = dspy.Predict(PerspectiveSignature)
        self.article_module = FullArticleCreationModule(target_words, max_iterations)

    def forward(self, topic):
        logging.info(f"üî¨ Research Phase: {topic}")

        # Fetch related topics
        related_topics = fetch_wikipedia_links(topic)

        # Generate TOC
        toc_data = self.generate_toc_module(
            topic=topic,
            related_topics=LinkData(links=related_topics).to_json(),
            rationale="Generate detailed TOC"
        )
        table_of_contents = toc_data.table_of_contents if hasattr(toc_data, 'table_of_contents') else "No TOC generated"
        logging.info("‚úì Table of contents generated")

        # Generate perspectives
        perspectives_output = self.perspective_predict(topic=topic)
        perspectives_list = perspectives_output.perspectives.split("\n") if hasattr(perspectives_output, 'perspectives') else []
        logging.info(f"‚úì {len(perspectives_list)} perspectives generated")

        # Conduct conversations
        logging.info("üí¨ Conversation Phase")
        conversation_history = [("Initial query", f"Introduction to {topic}")]
        formatted_history = ' '.join([f"{q}: {a}" for q, a in conversation_history])

        conversation_output = self.conversation_module(
            topic=topic,
            perspective=perspectives_output.perspectives if hasattr(perspectives_output, 'perspectives') else "",
            conversation_history=formatted_history
        )

        updated_history = conversation_history + [(conversation_output.question, conversation_output.answer)]
        logging.info(f"‚úì Conversations completed")

        # Generate article
        logging.info("‚úçÔ∏è  Article Generation Phase")
        prompt = "The impact of sustainable energy on global economies"
        generated_article, word_count, iterations = self.article_module.generate_full_article(
            topic, updated_history, prompt
        )

        return {
            "research": {
                "related_topics": related_topics,
                "table_of_contents": table_of_contents
            },
            "conversation": {
                "next_question": conversation_output.question,
                "answer": conversation_output.answer,
                "history": updated_history
            },
            "perspectives": perspectives_list,
            "article": generated_article,
            "metadata": {
                "word_count": word_count,
                "target_words": self.article_module.target_words,
                "iterations": iterations,
                "num_perspectives": len(perspectives_list),
                "num_conversations": len(updated_history),
                "timestamp": datetime.now().isoformat()
            }
        }

def save_results(results, output_file, format_type):
    """Save results to file in specified format"""
    output_path = Path(output_file)

    if format_type == "json":
        with open(output_path, 'w') as f:
            json.dump(results, f, indent=2)
        logging.info(f"üíæ Saved JSON to: {output_path}")

    elif format_type == "md":
        with open(output_path, 'w') as f:
            f.write(f"# {results.get('metadata', {}).get('topic', 'Article')}\n\n")
            f.write(f"**Word Count:** {results['metadata']['word_count']}\n")
            f.write(f"**Generated:** {results['metadata']['timestamp']}\n\n")
            f.write("---\n\n")
            f.write(results['article'])
        logging.info(f"üíæ Saved Markdown to: {output_path}")

    elif format_type == "txt":
        with open(output_path, 'w') as f:
            f.write(results['article'])
        logging.info(f"üíæ Saved text to: {output_path}")

def main():
    parser = argparse.ArgumentParser(
        description='STORM CLI - Comprehensive Research & Article Generation',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  # Basic usage (800 words)
  storm "Quantum Computing"

  # Long article (1500 words)
  storm "Artificial Intelligence" --words 1500

  # Short article with verbose output
  storm "Blockchain" --words 500 -v

  # Save to file
  storm "Climate Change" -o climate.md --format md

  # JSON output only
  storm "Space Exploration" --json-only

  # Custom iterations
  storm "Renewable Energy" --words 1200 --max-iterations 8

Environment:
  OPENROUTER_API_KEY    Required. Your OpenRouter API key.
        """
    )

    parser.add_argument('topic', help='Topic to research and write about')
    parser.add_argument('-w', '--words', type=int, default=800,
                       help='Target word count (default: 800)')
    parser.add_argument('-i', '--max-iterations', type=int, default=10,
                       help='Maximum generation iterations (default: 10)')
    parser.add_argument('-o', '--output', type=str,
                       help='Output file path (default: stdout)')
    parser.add_argument('-f', '--format', choices=['txt', 'md', 'json'],
                       default='txt', help='Output format (default: txt)')
    parser.add_argument('--json-only', action='store_true',
                       help='Output only JSON to stdout')
    parser.add_argument('-v', '--verbose', action='store_true',
                       help='Verbose logging output')
    parser.add_argument('--version', action='version', version='STORM CLI 1.0.0')

    args = parser.parse_args()

    # Setup logging
    setup_logging(args.verbose)

    # Print header
    if not args.json_only:
        print("\n" + "="*80)
        print("üå™Ô∏è  STORM CLI - Research & Article Generation")
        print("="*80)
        print(f"Topic: {args.topic}")
        print(f"Target: {args.words} words")
        print(f"Max iterations: {args.max_iterations}")
        print("="*80 + "\n")

    # Create module and generate
    try:
        module = StormResearchModule(
            target_words=args.words,
            max_iterations=args.max_iterations
        )

        results = module.forward(args.topic)
        results['metadata']['topic'] = args.topic

        # Output handling
        if args.json_only:
            print(json.dumps(results, indent=2))
        else:
            print("\n" + "="*80)
            print("üìä RESULTS")
            print("="*80)
            print(f"Word Count: {results['metadata']['word_count']}/{args.words}")
            print(f"Iterations: {results['metadata']['iterations']}")
            print(f"Perspectives: {results['metadata']['num_perspectives']}")
            print(f"Conversations: {results['metadata']['num_conversations']}")
            print("="*80 + "\n")

            print("="*80)
            print("üìÑ ARTICLE")
            print("="*80)
            print(results['article'])
            print("\n" + "="*80 + "\n")

            # Save to file if specified
            if args.output:
                save_results(results, args.output, args.format)
            else:
                print("üí° Tip: Use -o <file> to save to file")

        return 0

    except KeyboardInterrupt:
        print("\n\n‚ö†Ô∏è  Interrupted by user")
        return 130
    except Exception as e:
        logging.error(f"‚ùå Error: {e}")
        if args.verbose:
            import traceback
            traceback.print_exc()
        return 1

if __name__ == "__main__":
    sys.exit(main())
